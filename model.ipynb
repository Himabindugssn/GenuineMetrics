{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "mount_file_id": "1lz5X_RjjFQQD6JLNCmtg1Ahhh6OYdW3x",
      "authorship_tag": "ABX9TyMxjZXbCMEZdDLkdUHJHEFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himabindugssn/GenuineMetrics/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xXzJMJ7objc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9c17a826-be19-440c-f6fb-7c717c7e7fc0"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\r\u001b[K     |██████▍                         | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=3e43be27a53aefba8e930fa2abdcba95196890f0f154aa97826850d1434e1b88\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_-KrmDbk4G4"
      },
      "source": [
        "# importing the library for the analysis \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import glob \n",
        "import os \n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from keras import Sequential\n",
        "import emoji\n",
        "import re\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOw8Qx9T0-2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84d58b89-ee19-4564-ccec-52f91eab3cc9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ni2GR8PoUS-"
      },
      "source": [
        "twitter = pd.read_csv('/content/drive/My Drive/master_twitter_01.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFukz-YXq-M9"
      },
      "source": [
        "twitter.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQBS4Chl3N3K"
      },
      "source": [
        "twitter.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6MZgjLq3qr7"
      },
      "source": [
        "twitter.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpFBHyFc2-20"
      },
      "source": [
        "twitter= twitter.drop_duplicates(subset=\"Contents\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdJIdz2iolUg"
      },
      "source": [
        "twitter_emoji = pd.read_excel('/content/drive/My Drive/GSTN/Emoji_Popularity_index.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDXhJXdpxLs"
      },
      "source": [
        "# Getting the emoji for the tweets \n",
        "emoj_list =emoji.UNICODE_EMOJI\n",
        "print(len(emoj_list))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2poDsK9plT2"
      },
      "source": [
        "# checking the emoji mapping and their popularity index .....\n",
        "\n",
        "\n",
        "emoji_pop  = {}\n",
        "for row in range(len(twitter_emoji)) : \n",
        "    #print(twitter_emoji.loc[row, \"Emoji\"],twitter_emoji.loc[row, \"Polarity\"])\n",
        "    emoji_pop[twitter_emoji.loc[row, \"Emoji\"]] = twitter_emoji.loc[row, \"Polarity\"]\n",
        "\n",
        "print(emoji_pop)\n",
        "\n",
        "counter = 0\n",
        "for emoj in emoj_list: # emoj list is prepared from the emoji library \n",
        "    if emoj in emoji_pop.keys():\n",
        "        val = emoji_pop[emoj]\n",
        "        counter= counter + 1\n",
        "        \n",
        "print(\"Total emoji which got matched are \", counter )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESHuaAvepudQ"
      },
      "source": [
        "# User Defined function : Data cleansing and features creations .......\n",
        "\n",
        "def ind_creation(x):\n",
        "    if x ==\"POSITIVE\":\n",
        "        return 1\n",
        "    elif x  == \"NEGATIVE\":\n",
        "        return 0 \n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Creating the indicator for the further analysis .....\n",
        "twitter['Ind_Sentiment'] = twitter['Sentiment'].map(ind_creation) # calling the function which is created in the above \n",
        "\n",
        "# We would be dealing with indicator of 1 and 0 only ......\n",
        "twitter = twitter[twitter['Ind_Sentiment'] > -1 ]\n",
        "\n",
        "\n",
        "#checking the distribution of the indicator created for the analysis .....\n",
        "print(twitter['Ind_Sentiment'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfKhrbZWqAYL"
      },
      "source": [
        "# Creating the count of emoji for the analysis .....\n",
        "def split_count(text):\n",
        "    emoji_list = []\n",
        "    text = str(text)\n",
        "    pat = \"[^\\w\\s,]\"\n",
        "    if len(text)>0:\n",
        "        data = re.findall(pat, text)\n",
        "        for word in data:\n",
        "            for char1 in word: \n",
        "                if char1 in emoj_list:     \n",
        "                    emoji_list.append(word)\n",
        "                else:\n",
        "                    continue \n",
        "    else:\n",
        "        emoji_list.append(0)\n",
        "    return(len(emoji_list))\n",
        "\n",
        "# Creating the field for the further analysis \n",
        "twitter['cnt_emoji'] = twitter['Contents'].map(split_count)\n",
        "\n",
        "print(twitter['cnt_emoji'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27nBOca2qDRs"
      },
      "source": [
        "# checking the age for the further creations  \n",
        "print(twitter['Gender'].unique())\n",
        "\n",
        "# Using the gender for the classification \n",
        "\n",
        "def gender(x):\n",
        "    x = str(x)\n",
        "    if x.upper() == 'MALE':\n",
        "        return('MALE')\n",
        "    elif x.upper() == 'FEMALE':\n",
        "        return('FEMALE')\n",
        "    else:\n",
        "        return('UNK')\n",
        "\n",
        "\n",
        "# Creating the indicator variable for the analysis \n",
        "def ind_create(x,y):\n",
        "    if x.upper() == y.upper():\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Creating the three indicators variables for the analysis .....\n",
        "\n",
        "twitter['Gender'] = twitter['Gender'].map(gender)\n",
        "twitter['ind_male']= twitter['Gender'].map(lambda x :ind_create(x,'MALE'))\n",
        "twitter['ind_female']= twitter['Gender'].map(lambda x :ind_create(x,'FEMALE'))\n",
        "twitter['ind_unk']= twitter['Gender'].map(lambda x :ind_create(x,'UNK'))\n",
        "\n",
        "print(twitter['Gender'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S23LPc_MqFup"
      },
      "source": [
        "# Creating the total word counts of the tweet for the analysis .....\n",
        "def word_count(text):\n",
        "    text = str(text)\n",
        "    word_list = []\n",
        "    data = text.split(\" \")\n",
        "    return(len(data))\n",
        "\n",
        "twitter['word_count']= twitter['Contents'].map(word_count)\n",
        "\n",
        "# checking the counts of the word counts \n",
        "print(twitter[\"word_count\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx-ulmzCqHsW"
      },
      "source": [
        "def val(emoj):\n",
        "    if emoj in emoji_pop.keys():\n",
        "        val = int(emoji_pop[emoj])\n",
        "    else:\n",
        "        val = 0\n",
        "    return(val)\n",
        "\n",
        "# Creating the total popularity index for the emoji for the analysis ....\n",
        "\n",
        "def popularity_index(text):\n",
        "    pop_scr = []\n",
        "    try:\n",
        "        text = str(text).encode('latin-1').decode('utf-8').strip()\n",
        "        \n",
        "        pat = \"[^\\w\\s,]\"\n",
        "        if len(text)> 0:\n",
        "            data = re.findall(pat, text)\n",
        "            for word in data:\n",
        "                for char1 in word: \n",
        "                    if char1 in emoj_list :\n",
        "                        pop_scr.append(val(char1)) # storing the total popularity of the emoji used\n",
        "                    else:\n",
        "                        continue\n",
        "    except UnicodeDecodeError:\n",
        "        pop_scr.append(0)\n",
        "    # finding the sum of the list for the analysis ...\n",
        "    return(sum(pop_scr))\n",
        "\n",
        "# checking the emoji popularity variables for the further analysis ...\n",
        "twitter['emoji_popularity'] = twitter['Contents'].map(popularity_index)\n",
        "\n",
        "print(twitter['emoji_popularity'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zeAzXA6qKw8"
      },
      "source": [
        "################ Creating the function for generating some additional features  ##################\n",
        "\n",
        "    \n",
        "# Creating the function to identify max popularity given to tweets \n",
        "\n",
        "def popularity_strength(text):    \n",
        "    pop_scr = []\n",
        "    try:\n",
        "        text = str(text).encode('latin-1').decode('utf-8').strip() \n",
        "        pat = \"[^\\w\\s,]\"\n",
        "        if len(text)> 0:\n",
        "            data = re.findall(pat, text)\n",
        "            for word in data:\n",
        "                for char1 in word: \n",
        "                    if char1 in emoj_list :\n",
        "                        pop_scr.append(val(char1)) # storing the total popularity of the emoji used\n",
        "                    else:\n",
        "                        continue\n",
        "    except UnicodeDecodeError:\n",
        "        pop_scr.append(0) \n",
        "    # finding the sum of the list for the analysis ...\n",
        "    if len(pop_scr) ==0:\n",
        "        return(0)\n",
        "    elif len(pop_scr)==1:\n",
        "        return((pop_scr[0]))\n",
        "    else:\n",
        "        max_pop = max(pop_scr)\n",
        "        min_pop = min(pop_scr)\n",
        "        return((max_pop + min_pop))\n",
        "\n",
        "# Creating the field for the further analysis Manx values for the popularity index ...\n",
        "twitter['emoji_strength'] = twitter['Contents'].map(popularity_strength)\n",
        "print(twitter['emoji_strength'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM2OTobMqNtJ"
      },
      "source": [
        "# Further cleaning of the data for creating the embedding ......\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: str(x))                           # convert everything to lower case\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: x.lower())                           # convert everything to lower case\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'[@]\\w+', '', x))            # remove usertags \n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'[#\\(\\)\\[\\]]', '', x))       # remove brackets and # from hashtags\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'https?[\\w./:]+', '', x))    # remove urls\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'\\.{2,}', ' ', x))           # replacing 2+ dots to space\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'(.)\\1+', r'\\1\\1', x))       # replace multiple chars to 2 chars\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: re.sub(r'(-|\\')', '', x))            # removing -|\\''\n",
        "twitter['Contents'] = twitter['Contents'].apply(lambda x: x.strip())                           # remove trailing spaces\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIR2sICTqTJm"
      },
      "source": [
        "\n",
        "# Subsetting the data which we need to use only for the analysis ...\n",
        "\n",
        "X=twitter.loc[:,['Contents','cnt_emoji','word_count','ind_male','ind_female','ind_unk','Followers','Following','emoji_popularity','emoji_strength']]\n",
        "y=twitter['Ind_Sentiment']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsCO0VhpqU_4"
      },
      "source": [
        "# Splitting the data for the train and test \n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Checking the data for the further analysis \n",
        "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
        "print(ytest.count(),ytest.sum(), ytest.count() - ytest.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKH-1DTnqWmj"
      },
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tweet = tweet.lower()\n",
        "        tokens = tokenizer.tokenize(tweet)\n",
        "        tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
        "        tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
        "        tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIqMJGQuqYUx"
      },
      "source": [
        "vocab_size = 1000\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=vocab_size, tokenizer=tokenize)\n",
        "vectorizer.fit(Xtrain.loc[:,'Contents'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPGwBG6Tqcze"
      },
      "source": [
        "train_embeddings=vectorizer.transform(Xtrain.loc[:,'Contents'])\n",
        "test_embeddings = vectorizer.transform(Xtest.loc[:,'Contents'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF6NScBMqe8m"
      },
      "source": [
        "# We have to add the standarized values for additional features which we have created ...\n",
        "mean_train = Xtrain.loc[:,'cnt_emoji'].mean()\n",
        "std_train = Xtrain.loc[:,'cnt_emoji'].std()\n",
        "\n",
        "mean_train_cnt = Xtrain.loc[:,'word_count'].mean()\n",
        "std_train_cnt = Xtrain.loc[:,'word_count'].std()\n",
        "\n",
        "# Mean Count of the \"followers\" & \"following\"\n",
        "mean_train_f = Xtrain.loc[:,'Followers'].mean()\n",
        "std_train_f = Xtrain.loc[:,'Followers'].std()\n",
        "\n",
        "mean_train_f1 = Xtrain.loc[:,'Following'].mean()\n",
        "std_train_f1 = Xtrain.loc[:,'Following'].std()\n",
        "\n",
        "\n",
        "mean_train_p1 = Xtrain.loc[:,'emoji_popularity'].mean()\n",
        "std_train_p1 = Xtrain.loc[:,'emoji_popularity'].std()\n",
        "\n",
        "mean_train_s1 = Xtrain.loc[:,'emoji_strength'].mean()\n",
        "std_train_s1 = Xtrain.loc[:,'emoji_strength'].std()\n",
        "\n",
        "\n",
        "# Standarizing the emoji count in eaach tweets for the further ....\n",
        "Xtrain['std_cnt_emoji'] = Xtrain['cnt_emoji'].map(lambda x : (x - mean_train)/std_train)\n",
        "Xtest['std_cnt_emoji'] = Xtest['cnt_emoji'].map(lambda x : (x - mean_train)/std_train)\n",
        "\n",
        "Xtrain['std_cnt_word'] = Xtrain['word_count'].map(lambda x : (x - mean_train_cnt)/std_train_cnt)\n",
        "Xtest['std_cnt_word'] = Xtest['word_count'].map(lambda x : (x - mean_train_cnt)/std_train_cnt)\n",
        "\n",
        "\n",
        "Xtrain['std_followers'] = Xtrain['Followers'].map(lambda x: (x- mean_train_f)/std_train_f)\n",
        "Xtest['std_followers'] = Xtest['Followers'].map(lambda x: (x- mean_train_f)/std_train_f)\n",
        "\n",
        "Xtrain['std_following'] = Xtrain['Following'].map(lambda x: (x- mean_train_f1)/std_train_f1)\n",
        "Xtest['std_following'] = Xtest['Following'].map(lambda x: (x- mean_train_f1)/std_train_f1)\n",
        "\n",
        "Xtrain['std_following'] = Xtrain['Following'].map(lambda x: (x- mean_train_f1)/std_train_f1)\n",
        "Xtest['std_following'] = Xtest['Following'].map(lambda x: (x- mean_train_f1)/std_train_f1)\n",
        "\n",
        "Xtrain['std_emoji_pop'] = Xtrain['emoji_popularity'].map(lambda x: (x- mean_train_p1)/std_train_p1)\n",
        "Xtest['std_emoji_pop'] = Xtest['emoji_popularity'].map(lambda x: (x- mean_train_f1)/std_train_f1)\n",
        "\n",
        "Xtrain['std_emoji_strength'] = Xtrain['emoji_strength'].map(lambda x: (x- mean_train_s1)/std_train_s1)\n",
        "Xtest['std_emoji_strength'] = Xtest['emoji_strength'].map(lambda x: (x- mean_train_s1)/std_train_s1)\n",
        "\n",
        "print(\"Total \")\n",
        "print(Xtrain.columns)\n",
        "\n",
        "\n",
        "print(Xtrain['std_cnt_emoji'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_3ypnD7qh9T"
      },
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "print(\" checking train data .....\")\n",
        "\n",
        "Xtrain_embedded = hstack((train_embeddings,np.array(Xtrain['std_cnt_emoji'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['std_cnt_word'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['std_followers'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['std_following'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['ind_male'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['ind_female'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['ind_unk'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['std_emoji_pop'])[:,None])).tocsr()\n",
        "Xtrain_embedded = hstack((Xtrain_embedded,np.array(Xtrain['std_emoji_strength'])[:,None])).tocsr()\n",
        "\n",
        "\n",
        "print(\"checking test data ....\")\n",
        "Xtest_embedded = hstack((test_embeddings,np.array(Xtest['std_cnt_emoji'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['std_cnt_word'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['std_followers'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['std_following'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['ind_male'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['ind_female'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['ind_unk'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['std_emoji_pop'])[:,None])).tocsr()\n",
        "Xtest_embedded = hstack((Xtest_embedded,np.array(Xtest['std_emoji_strength'])[:,None])).tocsr()\n",
        "\n",
        "print(Xtest_embedded.shape) \n",
        "print(type(Xtest_embedded))\n",
        "\n",
        "print(Xtrain_embedded.shape) \n",
        "print(type(Xtrain_embedded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2uTbk7qqa8"
      },
      "source": [
        "twitter.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxbc4jdpq0CH"
      },
      "source": [
        "print(Xtrain_embedded.shape, ytrain.shape)\n",
        "ytrain_01 = ytrain.values.reshape(-1,1)\n",
        "print(ytrain_01.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cJgOvSz7au"
      },
      "source": [
        "# Building the model for the further analysis we have \n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqp_pykp0QPd"
      },
      "source": [
        "print(np.isnan(Xtrain_embedded.data).any())\n",
        "Xtrain_embedded.data[np.isnan(Xtrain_embedded.data)] = 0.001 # Missing value treatment \n",
        "print(Xtrain_embedded.shape)\n",
        "print(np.isnan(Xtrain_embedded.data).any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5GZLuFJ0VcY"
      },
      "source": [
        "#print(Xtrain_embedded.shape) \n",
        "\n",
        "\n",
        "# Building the model for the further analysis we have \n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import *\n",
        "\n",
        "# This is ANN model with 4  layers  with additional features 9 \n",
        "model_01=Sequential()\n",
        "model_01.add(Dense(128, input_dim=1009))\n",
        "model_01.add(BatchNormalization())\n",
        "model_01.add(Activation('relu'))\n",
        "model_01.add(layers.Dropout(0.4))\n",
        "\n",
        "model_01.add(Dense(64, activation='relu'))\n",
        "model_01.add(BatchNormalization())\n",
        "model_01.add(layers.Dropout(0.4))\n",
        "model_01.add(Dense(32, activation='relu'))\n",
        "model_01.add(BatchNormalization())\n",
        "model_01.add(layers.Dropout(0.4))\n",
        "model_01.add(Dense(16, activation='relu'))\n",
        "model_01.add(BatchNormalization())\n",
        "model_01.add(layers.Dropout(0.4))\n",
        "model_01.add(Dense(8, activation='relu'))\n",
        "model_01.add(BatchNormalization())\n",
        "model_01.add(layers.Dropout(0.4))\n",
        "model_01.add(Dense(1, activation='sigmoid'))\n",
        "model_01.summary()\n",
        "#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model_01.compile(optimizer='rmsProp',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l2uv5rY0YMD"
      },
      "source": [
        "# This is model without considering the additional features which has been created ....\n",
        "\n",
        "hist_01=model_01.fit(Xtrain_embedded.todense(),ytrain_01, epochs=50, batch_size= 512,validation_split= 0.2)\n",
        "history_01_dict = hist_01.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MOoLhq20aeD"
      },
      "source": [
        "print(Xtest.shape)\n",
        "print(ytest.shape)\n",
        "\n",
        "# Evaluating the model for the ana;lysis we have \n",
        "test_loss, test_acc = model_01.evaluate(Xtest_embedded.todense(), ytest)\n",
        "\n",
        "# Printing the test loss and test accuracy \n",
        "print(test_loss,test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnwM1gnL_0pb"
      },
      "source": [
        "#Run these codes first in order to install the necessary libraries and perform authorization\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcvW8tCzL0J0"
      },
      "source": [
        "#Mount your Google Drive:\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLXvqYvzMjz-"
      },
      "source": [
        "#After success run Drive FUSE program, you can create a directory Sentiment_Analysis and access your drive at /content/drive with using command\n",
        "import os\n",
        "os.mkdir(\"/content/drive/Sentiment_Analysis\")\n",
        "os.chdir(\"/content/drive/\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YviSx4E-QHHQ"
      },
      "source": [
        "#Append your path\n",
        "import sys\n",
        "sys.path.append('/content/drive/Sentiment_Analysis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXZEFBNdQmRN"
      },
      "source": [
        "#Now save the model in required directory\n",
        "model.save('/content/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVg1O61TQouo"
      },
      "source": [
        "#Check the content of the directory\n",
        "os.chdir(\"/content/drive/Sentiment_Analysis\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoVhCro1Qqua"
      },
      "source": [
        "#Code to load the saved model\n",
        "model = load_model('/content/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')\n",
        "print(\"Model Loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}