# -*- coding: utf-8 -*-
"""Extract the tweets- json, preprocess trial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0Zx_a1MZn3mVUE2u-kU6ZTmF_A8OpBl
"""

import numpy as np
a=[]
a.append((1,2))
a.append((3,4))
b= []
b.append((5,6))
b.append((7,8))


a= np.mean(a,axis=0)
b= np.mean(b,axis=0)
print(a,b)

for i,j in [a,b]:
  print(i,j)

"""## Trial for single element"""

import tweepy
from tweepy import OAuthHandler

# credentials 
consumer_key = 'JpdXgnUvOlkcMC7MtfseTc2tD'
consumer_secret = 'eJ1TlWGzm8DnqVKar1KaUNalZIue8ndnnzVTGZkflBTAK1AUvx'
access_key= '1154630220558696448-3NbWuZ6x0F8lNqMMuZzo1wfOvSck0l'
access_secret = 'N4O8E84GoaD0reldCU2vgBJjljCt71QVubg0Yi9PbvMoP'

# Authorization to consumer key and consumer secret 
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) 

# Access to user's access key and access secret 
auth.set_access_token(access_key, access_secret) 

# Calling api 
api = tweepy.API(auth)

!pip install emoji
import emoji

def extract_emojis(s):
  a=''.join(c for c in s if c in emoji.UNICODE_EMOJI)
  if(a==''):
    return None
  else:
    return a

#Installing emot library
!pip install emot
#Importing libraries
import re
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
# Function for converting emojis into word
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
    return text

col=["GST" , "Infosys","askGST_GoI" , "@askGST_GoI" , "askGSTech" , "@askGSTech" ,"@GST_Council" , "GST_Council" , "GSTN" ,"ITC01" , "ITC-01" , 
     "ITC 01" , "ITC 1" , "ITC1" , "ITC02" , "ITC-02" , "ITC 02" , "ITC 2" , "ITC2" , "ITC03" , "ITC-03" , "ITC 03" , "ITC 3" , "ITC3" , "ITC04" , 
     "ITC-04" , "ITC 04" , "ITC 4" , "ITC4" , "TRAN1" , "TRAN01" , "TRAN 01" , "TRAN-01" , "TRAN2" , "TRAN02" , "TRAN 02" , "TRAN-02" , "GSTR1" , 
     "GSTR01" , "GSTR-01" , "GSTR-1" , "GSTR3B" , "GSTR 3B" , "3B" , "GSTR4" , "GSTR04" , "GSTR 4" ,
     "GSTR5" , "GSTR05" , "GSTR 5" , "GSTR6" , "GSTR06" , "GSTR 6" , "GSTR11" , "GSTR 11" , "Payment","Tax" , "Challan" , "challan"
      , "Filing" , "Save" , "Submit" , "Registration" , "Regn" , "Register" , "Migration" ,"Cancellation", "Provisional ID" , "Prov ID" , "ProvID" ,"LUT" ,
      "Application Reference Number" , "ARN" ,"Refund" , "GSTR2A" , "GSTR 2A" , "2A" , "TRN" , "transaction reference number" , "Upload" ,"Invoice" , 
     "Refund" , "Offline" , "Utility" , "RFD" , "RFD01" , "RFD01A" , "RFD 01" , "RFD 01A" ,"RFD02" , "RFD 2" , "RFD 02" ,"Appeal" , "recovery", "DRC" ,
     "OTP" , "SMS" , "email" 
     , "mail" , "portal" , "gst.gov.in" , "gst portal" , "Registration" , "CGST" , "IGST" , "SGST" , "exempt" , "nil rated" , "zero rate" , "reverse" , "RCM" ,
     "Casual Taxpayer" , "ISD" , "TDS" , "UIN" , "Non-resident" , "Enrolment" , "business" , "GSTP" , "GST Practitioner" , "Opt in" , "Opt out" , 
     "Cancellation" , "Revocation" , "CMP" , "Dashboard" , "Suo Moto" , "Reg 30" , "Reg 29" , "Reg 16" , "Core amendment" , "Non core" , "Composition" , 
     "Non-core" , "fields" , "RC" , "OIDAR" , "NRTP" , "B2B" , "Supplies" , "9" , "9A" , "9C" , "NEFT" , "RTGS" , "OTC" , "payment" , 
     "Electronic cash ledger" , "Liability ledger" , "ITC ledger" , "PMT" , "Grievance redressal portal" , "GRP" , "Self service portal" , 
     "Transition form" , "tran" , "Services" , "SEZ" , "Inter State" , "Intra State" , "Refund" , "Annual return" , "Preparation" , "Non filers" , 
     "Fraud" , "Evasion" , "Notice" , "Assessment" , "Adjudication" , "Jurisdiction" , "Appeal" , "audit" , "CAG" , "Demand Order" , "Auto approval" , 
     "Advance ruling" , "Auth" , "GIC" , "IT Committee" , "GOM" , "group of ministers" , "Appellate" , "authority" , "Recovery" , "Debit" , "E way bill" , 
     "Seizure" , "eway" , "bill" , "Inspection" , "NIL Return" , "Challan" , "ARA" , "PAN" , "APL" , "EODB" , "ease of doing business" , "LUT" , "Portal" 
     , "MSME" , "SME" , "Prosecution" , "Notification" , "Compounding" , "Offence" , "Unregistered" , "Case" , "State" , "Center" , "Centre" , "CBIC" , 
     "CBEC" , "Council" , "FM" , "Jaitley"
]

import time
  #@title Scrap tweets:

  #keeps count of total number of tweets obtained 
  no_of_tweets=0
  #list to store the tweets' attributes
  tweet_attr=[]
  user_mention_list=[]
  for keyword in col:
    query=keyword
    
    #@markdown For a regular developer, the maximum tweets returned are up to 3,200 of a user's most recent Tweets.
    max_tweets = 50  #@param {type: "slider", min:0, max:3200,step:50}

    #@markdown Enter number of tweets per query is required
    max_tweets_per_query = 26  #@param {type: "slider", min:1, max:100,step:5}

    #@markdown Tweets tweeted after the following date will be returned 
    date = '2020-01-01'  #@param {type: "date"}
  
    #if the number of tweets obtained is less than required, run the below loop
    while(no_of_tweets < max_tweets):
      try:
          query=query+' -filter:retweets' 

          #query 
          tweets_list=api.search(q=query,count=max_tweets_per_query,lang="en", since=date,tweet_mode='extended') #tweet_mode to get full text

          #collecting attributes for each tweet obtained from the query
          for tweets in tweets_list:
              dates=tweets.created_at.date() #tweet date
              times=tweets.created_at.time() #tweet time
              tweet_id= (tweets._json['id']) #tweet ID
              tweet_id_str=(tweets._json['id_str']) #tweet ID in string
              text=tweets.full_text # full text of tweet - #for tweets.text - only a few words are being shown)
              text_with_converted_emoji=convert_emojis(text) #convert emoji into text
              emoji_from_tweets= extract_emojis(text) #emojis extracted from the text 
              source=tweets.source #Utility used to post the Tweet, as an HTML-formatted string
              truncated= tweets.truncated #Indicates whether the value of the text parameter was truncated
              in_reply_to_status_id= tweets.in_reply_to_status_id #If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s ID.
              in_reply_to_status_id_str= tweets.in_reply_to_status_id_str #the string representation of the original Tweet’s ID 
              in_reply_to_user_id=tweets.in_reply_to_user_id #If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s author ID.
              in_reply_to_user_id_str=tweets.in_reply_to_user_id_str #string representation of the original Tweet’s author ID.
              in_reply_to_screen_name=tweets.in_reply_to_screen_name #If the represented Tweet is a reply, this field will contain the screen name of the original Tweet’s author.
              user_id= tweets.user.id        #tweet author's ID
              user_id_str=tweets.user.id_str #tweet author's ID in string form
              user_name=tweets.user.name     #tweet author's name
              screen_name=tweets.user.screen_name #The screen name, handle, or alias that this user identifies themselves with
              location=tweets.user.location #location of user
              user_url=tweets.user.url       #tweet author's URL
              description=tweets.user.description  #The user-defined UTF-8 string describing their account
              protected=tweets.user.protected #When true, indicates that this user has chosen to protect their Tweets.
              verified=tweets.user.verified #When true, indicates that the user has a verified account
              followers_count=tweets.user.followers_count #The number of followers this account currently has. Under certain conditions of duress, this field will temporarily indicate “0”.
              friends_count=tweets.user.friends_count #The number of users this account is following (AKA their “followings”). Under certain conditions of duress, this field will temporarily indicate “0”.
              listed_count=tweets.user.listed_count #The number of public lists that this user is a member of. 
              favourites_count=tweets.user.favourites_count #The number of Tweets this user has liked in the account’s lifetime.
              statuses_count=tweets.user.statuses_count #number of Tweets (including retweets) issued by the user
              date_user_account=tweets.user.created_at.date() #user account created on that date
              time_user=tweets.user.created_at.time() #user account created on that time

              if(tweets._json['place']!=None):
                place_id=tweets.place.id #string ID representing this place
                place_url=tweets.place.url #URL representing the location of additional place metadata for this place
                place_type=tweets.place.place_type #The type of location represented by this place.
                place_short_name=tweets.place.name #Short human-readable representation of the place’s name.
                place_full_name=tweets.place.full_name #Full human-readable representation of the place’s name
                country_code=tweets.place.country_code #country code
                country=tweets.place.country #country name
              else:
                place_id=None
                place_url=None 
                place_type=None 
                place_short_name=None 
                place_full_name=None 
                country_code=None 
                country=None 

              retweet_count=tweets.retweet_count #Number of times this Tweet has been retweeted. 
              favorite_count=tweets.favorite_count #Indicates approximately how many times this Tweet has been liked by Twitter users.
              if(len(tweets.entities['hashtags'])==1):
                  hashtags_text=(tweets.entities['hashtags'][0]['text']) #Name of the hashtag, minus the leading ‘#’ character in the tweet text
              else:
                  hashtags_text= None

              user_mentions_count= len(tweets.entities['user_mentions']) #number of user mentions

              if(len(tweets.entities['user_mentions'])>=1): #the lists stores the data of user mentions
                  for i in range (len(tweets.entities['user_mentions'])):
                    screen_name=tweets.entities['user_mentions'][i]['screen_name'] # screen name of the referenced user
                    mention_name=tweets.entities['user_mentions'][i]['name']
                    id_mention=tweets.entities['user_mentions'][i]['id']
                    id_str_mention=tweets.entities['user_mentions'][i]['id_str']
                    
                    user_mention_list.append([tweets.id,tweets.user.id,screen_name,mention_name,id_mention,id_str_mention]) #appended the tweet id and  user_id of the tweet along with user mentions so that incase a database is made, these can be used as primary keys
                  
              favorited= tweets.favorited #Indicates whether this Tweet has been liked by the authenticating user. 
              retweeted=tweets.retweeted #Indicates whether this Tweet has been Retweeted by the authenticating user. 
              #there are some more attributes as well which are causing problem ! "status object doen't have this feature"
              #some tweets have them while some don't !
              #possibly_sensitive = tweets.possibly_sensitive #an indicator that the URL contained in the Tweet may contain content or media identified as sensitive content. 
              topic= query[:query.find('-')].strip() #query keyword used for extarcting this particular tweet
              tweet_attr.append([dates,times,tweet_id,tweet_id_str,text,text_with_converted_emoji,emoji_from_tweets,source,truncated,in_reply_to_status_id,
                                  in_reply_to_status_id_str,in_reply_to_user_id,in_reply_to_user_id_str,in_reply_to_screen_name,user_id,user_id_str,
                                  user_name,screen_name,location,user_url,description,protected,verified,followers_count,friends_count,listed_count,
                                  favourites_count,statuses_count,date_user_account,time_user,place_id,place_url,place_type,place_short_name,
                                  place_full_name,country_code,country,hashtags_text,user_mentions_count,retweet_count,favorite_count,favorited,retweeted,topic])
      except tweepy.TweepError as e:
            # Just exit if any error
            print("some error : " + str(e))
            break

      #add the number of tweets to the total count
      no_of_tweets+=len(tweets_list)
      if(no_of_tweets>=3200):
        print("I am sleeping")
        time.sleep(901) #wait for 15.1 min if the no. of tweets is ~3200
        no_of_tweets=0 #reset the value fo tweets 
        
      print("current no of tweets",no_of_tweets,"size of the list",len(tweet_attr),"size of the user_mentions list",len(user_mention_list),"for keyword",keyword)

len(tweet_attr[0])

len(user_mention_list[0])

columns1=['dates','times','tweet_id','tweet_id_str','text','text_with_converted_emoji','emoji_from_tweets','source','truncated','in_reply_to_status_id',
                              'in_reply_to_status_id_str','in_reply_to_user_id','in_reply_to_user_id_str','in_reply_to_screen_name','user_id','user_id_str',
                              'user_name','screen_name','location','user_url','description','protected','verified','followers_count','friends_count','listed_count',
                              'favourites_count','statuses_count','date_user_account','time_user','place_id','place_url','place_type','place_short_name',
                              'place_full_name','country_code','country','hashtags_text','user_mentions_count','retweet_count', 'favorite_count','favorited','retweeted','topic'
]
len(columns1)

import pandas as pd
columns1=['dates','times','tweet_id','tweet_id_str','text','text_with_converted_emoji','emoji_from_tweets','source','truncated','in_reply_to_status_id',
                              'in_reply_to_status_id_str','in_reply_to_user_id','in_reply_to_user_id_str','in_reply_to_screen_name','user_id','user_id_str',
                              'user_name','screen_name','location','user_url','description','protected','verified','followers_count','friends_count','listed_count',
                              'favourites_count','statuses_count','date_user_account','time_user','place_id','place_url','place_type','place_short_name',
                              'place_full_name','country_code','country','hashtags_text','user_mentions_count','retweet_count', 'favorite_count','favorited','retweeted','topic'
]
columns2=['tweets_id','tweets_user_id','screen_name','mention_name','id_mention','id_str_mention']
df_1= pd.DataFrame(tweet_attr,columns=columns1)
df_2=pd.DataFrame(user_mention_list,columns=columns2)

df_1.to_csv('trial_tweets.csv')
df_2.to_csv('trial_mention.csv')

df_1.head()



df_2.head()

# lists[] should be placed where the tweet_attr is placed ! 


dates=tweets.created_at.date() #tweet date
times=tweets.created_at.time() #tweet time
tweet_id= (tweets._json['id']) #tweet ID
tweet_id_str=(tweets._json['id_str']) #tweet ID in string
text=tweets.full_text # full text of tweet - #for tweets.text - only a few words are being shown)
text_with_converted_emoji=convert_emojis(text) #convert emoji into text
emoji_from_tweets= extract_emojis(text) #emojis extracted from the text 
source=tweets.source #Utility used to post the Tweet, as an HTML-formatted string
truncated= tweets.truncated #Indicates whether the value of the text parameter was truncated
in_reply_to_status_id= tweets.in_reply_to_status_id #If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s ID.
in_reply_to_status_id_str= tweets.in_reply_to_status_id_str #the string representation of the original Tweet’s ID 
in_reply_to_user_id=tweets.in_reply_to_user_id #If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s author ID.
in_reply_to_user_id_str=tweets.in_reply_to_user_id_str #string representation of the original Tweet’s author ID.
in_reply_to_screen_name=tweets.in_reply_to_screen_name #If the represented Tweet is a reply, this field will contain the screen name of the original Tweet’s author.
user_id= tweets.user.id        #tweet author's ID
user_id_str=tweets.user.id_str #tweet author's ID in string form
user_name=tweets.user.name     #tweet author's name
screen_name=tweets.user.screen_name #The screen name, handle, or alias that this user identifies themselves with
location=tweets.user.location #location of user
user_url=tweets.user.url       #tweet author's URL
description=tweets.user.description  #The user-defined UTF-8 string describing their account
protected=tweets.user.protected #When true, indicates that this user has chosen to protect their Tweets.
verified=tweets.user.verified #When true, indicates that the user has a verified account
followers_count=tweets.user.followers_count #The number of followers this account currently has. Under certain conditions of duress, this field will temporarily indicate “0”.
friends_count=tweets.user.friends_count #The number of users this account is following (AKA their “followings”). Under certain conditions of duress, this field will temporarily indicate “0”.
listed_count=tweets.user.listed_count #The number of public lists that this user is a member of. 
favourites_count=tweets.user.favourites_count #The number of Tweets this user has liked in the account’s lifetime.
statuses_count=tweets.user.statuses_count #number of Tweets (including retweets) issued by the user
date_user_account=tweets.user.created_at.date() #user account created on that date
time_user=tweets.user.created_at.time() #user account created on that time

if(tweets._json['place']!=None):
  place_id=tweets.place.id #string ID representing this place
  place_url=tweets.place.url #URL representing the location of additional place metadata for this place
  place_type=tweets.place.place_type #The type of location represented by this place.
  place_short_name=tweets.place.name #Short human-readable representation of the place’s name.
  place_full_name=tweets.place.full_name #Full human-readable representation of the place’s name
  country_code=tweets.place.country_code #country code
  country=tweets.place.country #country name
else:
  place_id=None
  place_url=None 
  place_type=None 
  place_short_name=None 
  place_full_name=None 
  country_code=None 
  country=None 

retweet_count=tweets.retweet_count #Number of times this Tweet has been retweeted. 
favorite_count=tweets.favorite_count #Indicates approximately how many times this Tweet has been liked by Twitter users.
if(len(tweets.entities['hashtags'])==1):
    hashtags_text=(tweets.entities['hashtags'][0]['text']) #Name of the hashtag, minus the leading ‘#’ character in the tweet text
else:
    hashtags_text= None

user_mentions= len(tweets.entities['user_mentions']) #number of user mentions

if(len(tweets.entities['user_mentions'])>=1): #the lists stores the data of user mentions
    for i in range (len(tweets.entities['user_mentions'])):
      screen_name=tweets.entities['user_mentions'][i]['screen_name'] # screen name of the referenced user
      mention_name=tweets.entities['user_mentions'][i]['name']
      id_mention=tweets.entities['user_mentions'][i]['id']
      id_str_mention=tweets.entities['user_mentions'][i]['id_str']
      
      lists.append([tweets.id,tweets.user.id,screen_name,mention_name,id_mention,id_str_mention]) #appended the tweet id and  user_id of the tweet along with user mentions so that incase a database is made, these can be used as primary keys
    
favorited= tweets.favorited #Indicates whether this Tweet has been liked by the authenticating user. 
retweeted=tweets.retweeted #Indicates whether this Tweet has been Retweeted by the authenticating user. 
possibly_sensitive=tweets.possibly_sensitive #an indicator that the URL contained in the Tweet may contain content or media identified as sensitive content. 
topic= query[:query.find('-')].strip() #query keyword used for extarcting this particular tweet



"""# PreProcess"""

from google.colab import files
files.upload()

import numpy as np
import pandas as pd

df=pd.read_csv('tweets_keywords0.csv')

df.describe()

df.shape

df.head()

import matplotlib.pyplot as plt

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['column_name', 'missing_count']
missing_df = missing_df.loc[missing_df['missing_count']>0]
missing_df = missing_df.sort_values(by='missing_count')

ind = np.arange(missing_df.shape[0])
width = 0.2
fig, ax = plt.subplots(figsize=(5,10))
rects = ax.barh(ind, missing_df.missing_count.values, color='red')
ax.set_yticks(ind)
ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')
ax.set_xlabel("Count of missing values")
ax.set_title("Number of missing values in each column")
plt.show()

def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns

missing_values_table(df)

"""using preprocessor"""

!pip install tweet-preprocessor

import preprocessor as p

def preprocess(df):
  text_preprocessor=df['text']
  text_preprocessor=p.clean(text_preprocessor)
  text_emoji_preprocessor=df['text_with_converted_emoji']
  text_emoji_preprocessor=p.clean(text_emoji_preprocessor)

preprocess(df)

fd

"""#Preprocess on normal text :)
by the time those data loads -_-
"""

import re
import pandas 
import numpy

# for capital words 
sent="hi ! I am GOOD girl. I am MAD"
caps = re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', sent,re.MULTILINE) 
print(caps)

# for words which are followed by "exclamation mark"
# ! ? 
s="hi? good!?????"
caps=re.findall(r'(\w+)?', s)
for i in caps:
  if(i!=''):
    print(i)

"""https://dair.ai/Exploratory_Data_Analysis_for_Text_Data/"""

import pandas as pd
import seaborn as sns

# avg text length (plz take it after removing a, an the etc)
s=" a a a big small top dfghjkldfghbj dxfcgvhbjnkmzsdxfcgvhbjnkxfcgvhbjnk the this then why a a an"
print(s.split())
l=[]
for x in s.split():
  l.append(len(x))

l=pd.DataFrame(l)
sns.boxplot(x=l)

# removes the stop owrds 
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
stops =  set(stopwords.words('english')+['com'])
co = CountVectorizer(stop_words=stops)
counts = co.fit_transform(s.split())
pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)

#remove punctuations, special characters
s="hi! I am Himu."
s=s.replace("[^a-zA-Z#]", " ")
print(s)

from google.colab import files
files.upload()

df=pd.read_csv('tweets_keywords1.csv')

dff=df.copy()

sent=dff['text_with_converted_emoji'][10]
print(sent)
caps = re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', sent,re.MULTILINE) 
print(caps)

!pip install wordcloud
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS 

comment_words = '' 
stopwords = set(STOPWORDS) 
  
# iterate through the csv file 
for val in dff['text_with_converted_emoji']: 
      
    # typecaste each val to string 
    val = str(val) 
  
    # split the value 
    tokens = val.split() 

    comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800, 
                background_color ='white', 
                stopwords = stopwords, 
                min_font_size = 10).generate(comment_words) 
  
# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
  
plt.show()

"""convert the capital words into dataframe"""

caps=[]
for i in dff['text_with_converted_emoji']:
  sent=str(i)
  k=(re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', sent,re.MULTILINE))
  for j in k:
    caps.append(j)
print(caps)
caps=pd.DataFrame(data=caps,columns=["name"])

"""word cloud of capital letters"""

comment_words = '' 
stopwords = set(STOPWORDS) 
  
# iterate through the csv file 
for val in caps["name"]: 
      
    # typecaste each val to string 
    val = str(val) 
  
    # split the value 
    tokens = val.split() 
      
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
      comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800, 
                background_color ='white', 
                stopwords = stopwords, 
                min_font_size = 10).generate(comment_words) 
  
# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
  
plt.show()

!pip install tweet-preprocessor

#convert to lower text
for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i]=dff['text_with_converted_emoji'][i].lower()

#remove numbers 
import re
for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i] = re.sub(r'\d+','',dff['text_with_converted_emoji'][i])

!pip install str

#remove punctuations
import str
for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i]=dff['text_with_converted_emoji'][i].translate(maketrans('',''),str.punctuation)

from str import maketrans
s ="string. With. Punctuation?"
s=s.translate(maketrans("",""),str.punctuation)
print(s)

import re
expression = "doing himu ing"
print(re.findall(r'\b(\w+ing\b)', expression))

expression = "doing! !himu ing!! I am Bindu it's himu."
print(re.findall(r'.', expression))
print(re.findall(r'.*', expression))
print(re.findall(r'\w+', expression))
print(re.findall(r'.', expression))
print(re.findall(r'.', expression))
print(re.findall(r'.', expression))
print(re.findall(r'.', expression))
print(re.findall(r'.', expression))
# do not remove ' before as it has "it's" type of words and we have to do punctuation remove afterwards

#non word characters
text = "The film, '@Pulp Fiction' was ? released in % $ year 1994."
result = re.sub(r"\w", "", text, flags=re.I)
print(result)
result = re.sub(r"\W", "", text, flags=re.I)
print(result)

#remove the punctuations 
text = "The!! film:, '@Pulp Fiction' was ? released _ in % $ year 1994."
result = re.sub(r"[,@\'?\.$%_:]", "", text, flags=re.I) #include all the punctuations inside the square brackets  
print(result)

text = "The!! film:, '@Pulp Fiction' was ? released _ in % $ year 1994 it's."
result = re.sub(r"[,@\?\.$%_:]", "", text, flags=re.I) #include all the punctuations inside the square brackets  
print(result)
result=re.sub(r'[\']'," ",result,flags=re.I) #change it's to it s 
print(result)

text=re.split('[\']',text)
print(text)

#remove multiple spaces
text = "  The film      Pulp Fiction      was released in   year 1994."
result = re.sub(r"\s+"," ", text, flags = re.I)
print(result)

expression = "doing!! himu! ing"
print(re.findall(r'\w+!', expression)) # did this myself !! yay!!!!

"""after converting into tokens and calculating the tfidf, make a stop words list!"""

i="@_g#yhujkl: ; dfgvhbjkml,;."
i="Thanks to @Racheldoesstuff who brought the problems to light years back &amp; to @TessParas as the new program director. I've only heard great things about the showcase the past few years &amp; it would be unfair to not highlight what a positive thing it is now.  https://t.co/UgcFxnL7zc"
i=re.sub(r"[\,\@\'\.\$\%\_\*\&\#\:\;]", "", i, flags=re.I)
print(i)
i=re.sub(r"[-,@\'@#&*\.$%_]", "",i, flags=re.I)
i=re.sub(r"[()]","",i,flags=re.I)
i=re.sub(r"[\\]","",i,flags=re.I)
print(i)

i= "https.himu. cfcgvbhnjmk,"
i=re.sub(r"[http]","",i,flags=re.I)
print(i)

# remove all punctuations except '!' and '?'
print(dff['text_with_converted_emoji'][1])
for i in range(len(dff['text_with_converted_emoji'])):
  
  dff['text_with_converted_emoji'][i]=re.sub(r"[\-\,\@\'\.\$\%\_\*\&\#\;\:]", "", dff['text_with_converted_emoji'][i], flags=re.I)
  dff['text_with_converted_emoji'][i]=re.sub(r"[()]","",dff['text_with_converted_emoji'][i],flags=re.I)
  dff['text_with_converted_emoji'][i]=re.sub(r"[\\]","",dff['text_with_converted_emoji'][i],flags=re.I)
print(dff['text_with_converted_emoji'][1])

#after everything is done remove the exclamation and question mark :)
for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i]=re.sub(r"[\?\!]", "", dff['text_with_converted_emoji'][i], flags=re.I)
print(dff['text_with_converted_emoji'][1])

excla=[]
for i in dff['text_with_converted_emoji']:
  i=re.findall(r'\w+!',i)
  for j in i:
    excla.append(j)

print(excla)
excla=pd.DataFrame(excla,columns=['name'])

excla.head()

#word cloud of words with exclamation marks
comment_words = '' 
stopwords = set(STOPWORDS) 
  
# iterate through the csv file 
for val in excla['name']: 
      
    # typecaste each val to string 
    #val = str(val) 
  
    # split the value 
    tokens = val.split() 
      
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
      comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800, 
                background_color ='white', 
                stopwords = stopwords, 
                min_font_size = 10).generate(comment_words) 
  
# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
  
plt.show()

#question mark
excla=[]
for i in dff['text_with_converted_emoji']:
  i=re.findall(r'\w+?',i)
  for j in i:
    excla.append(j)
excla=pd.DataFrame(excla,columns=['name'])
comment_words = '' 
stopwords = set(STOPWORDS) 
  
# iterate through the csv file 
for val in excla['name']: 
      
    # typecaste each val to string 
    #val = str(val) 
  
    # split the value 
    tokens = val.split() 
      
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
      comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800, 
                background_color ='white', 
                stopwords = stopwords, 
                min_font_size = 10).generate(comment_words) 
  
# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
  
plt.show()

print(dff['text_with_converted_emoji'][1])

dff['text_with_converted_emoji'].head()

for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i] =re.sub(r"\s+"," ",dff['text_with_converted_emoji'][i], flags = re.I)

dff['text_with_converted_emoji'].head()

dff['text_with_converted_emoji'][1]

text="now https//tco/ugcfxnlzc"
#text=re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '',text, flags=re.MULTILINE)
#text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
#text = re.sub(r'(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*', '', text, flags=re.MULTILINE)
#text=re.sub('http[s]?://\S+', '', text)
text



#remove the url
for i in range(len(dff['text_with_converted_emoji'])):
  dff['text_with_converted_emoji'][i]=re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '',dff['text_with_converted_emoji'][i], flags=re.MULTILINE)

dff['text_with_converted_emoji'][1]

"""how to remove the url???"""

tokens=[]
for i in dff['text_with_converted_emoji']:
  text=re.findall('\w+',i)
  for j in text:
    tokens.append(j)

len(tokens)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
stops =  set(stopwords.words('english')+['com'])
co = CountVectorizer(stop_words=stops)
counts = co.fit_transform(tokens)
tok=pd.DataFrame(data=counts.sum(axis=0),columns=co.get_feature_names())
count_tokens=tok.T.sort_values(0,ascending=False)

count_tokens.head()

count_tokens[0]

type(count_tokens)
print(count_tokens.shape)

t=[]
for i in count_tokens.index:
  t.append(i)
df2=count_tokens.assign(count=count_tokens[0],name=t)

df2.head()

df2.drop(columns=[0])

df2.head(25)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
stops =  set(stopwords.words('english')+['com'])
co = CountVectorizer(stop_words=stops)
counts = co.fit_transform(tokens)
pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)

from google.colab import files
files.upload()

import pandas as pd

df=pd.read_csv('tweets_keywords7.csv')
df.head()

"""# using preprocessor"""

from google.colab import files
files.upload()

import pandas as pd

df=pd.read_csv('tweets_keywords0.csv')

df.head()

text1=df['text_with_converted_emoji'][1]
text2=df['text_with_converted_emoji'][50]
text3=df['text_with_converted_emoji'][500]

!pip install tweet-preprocessor
!pip install wordcloud

# required libraries
import re
import pandas as pd
import numpy as np
import seaborn as sns
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

import preprocessor as p
res=p.clean(text1)
print(res,'\n',text1)

abc="Hi isn't shouldn't it's U.S. Europe :) https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/"
print(p.clean(abc))

"""


*   p.clean -> U.S. , it's -> will be retained, url removed , :) removed 
*   use our predefined cleaning , but don't use ' this thing in punctuation ..while tokenizing it's useful !
* use tokenize with nltk !



"""

df['clean']=df['text_with_converted_emoji']

def clean_tweets(lists):
  

print(tokens_without_sw)
  
  for i in range(len(lists)):
    
    #lists[i]=str(lists[i])
    #remove URL
    lists[i]=p.clean(lists[i])

    #convert to lower text
    lists[i]=lists[i].lower()

    #remove numbers 
    lists[i] = re.sub(r'\d+','',lists[i])
  
    #remove punctuations
    lists[i]=re.sub(r"[\"#$%&()*+,-./:;<=>?@[\]^_`{|}~]", "",lists[i], flags=re.I)
      #lists[i] = re.sub(r'[\']'," ",lists[i],flags=re.I) #change it's to it s but isn't to isn t -_-

    #remove multiple spaces
    lists[i]=re.sub(r"\s+"," ", lists[i], flags = re.I)

clean_tweets(df['clean'])

print(df['clean'][110])
print(df['text_with_converted_emoji'][110])

#tokenize tweets :
df['tokenized']=df['clean']
tokenizer=nltk.tokenize.TreebankWordTokenizer()
for i in range(len(df['tokenized'])):
  df['tokenized'][i]=tokenizer.tokenize(df['tokenized'][i])

df['tokenized'][1]

"""trial!!"""

import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer

text= df['clean'][1]
tokenized_text=sent_tokenize(text)
#using CountVectorizer and removing stopwords in english language
cv1= CountVectorizer(lowercase=True,stop_words='english')
#fitting the tonized senetnecs to the countvectorizer
text_counts=cv1.fit_transform(tokenized_text)

print(text_counts)

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer = WordNetLemmatizer()
sentence = "The striped bats are hanging on their feet for best"
words = word_tokenize(sentence)
for w in words:
  print(w, " : ", lemmatizer.lemmatize(w))

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer = WordNetLemmatizer()
df['lemma']=df['clean']
for i in range(len(df['clean'])):
  df['lemma'][i]=word_tokenize(df['clean'][i])

df['lemma'][4]

df['tokenized'][4]

df.head()

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

text = "Nick likes to play football, however he is not too fond of tennis."
text_tokens = word_tokenize(text)

tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]

print(tokens_without_sw)

df['stopremove']=df['tokenized']
for i in range(len(df['tokenized'])):
  df['stopremove'][i]=[word for word in df['stopremove'][i] if not word in stopwords.words()]
  print(i,"done")

df.shape

df['tokenized'][2]

df['stopremove'][1]

df['lemma'][2]

# Creating the Bag of Words model 
word2count = {} 
for data in dataset: 
	words = nltk.word_tokenize(data) 
	for word in words: 
		if word not in word2count.keys(): 
			word2count[word] = 1
		else: 
			word2count[word] += 1

import heapq 
freq_words = heapq.nlargest(100, word2count, key=word2count.get)

X = [] 
for data in dataset: 
	vector = [] 
	for word in freq_words: 
		if word in nltk.word_tokenize(data): 
			vector.append(1) 
		else: 
			vector.append(0) 
	X.append(vector) # for each sentence in a paragh !
X = np.asarray(X)

df['tokenized'][1]



from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer()X = vectorizer.fit_transform(allsentences)print(X.toarray())

from sklearn.feature_extraction.text import CountVectorizer
vectorizer=CountVectorizer()
text=['hi','I', 'am','I','am', 'Himabindu']
X=vectorizer.fit_transform(text)
print (X.toarray())

df['bow']=df['stopremove']
from sklearn.feature_extraction.text import CountVectorizer
vectorizer=CountVectorizer()
for i in range(len(df['bow'])):
  df['bow'][i]=vectorizer.fit_transform(df['tokenized'][i])

print(df['bow'][2])

print(df['stopremove'][2])

from google.colab import files 
files.upload()

df=pd.read_csv('tweets_keywords1.csv')

df.head()

# import TextBlob 
from textblob import TextBlob 

gfg = TextBlob("GFG is a good compny and alays valule ttheir employes.") 

# using TextBlob.correct() method 
gfg = gfg.correct() 

print(gfg)



"""# new preprocess and EDA"""

# import files
from google.colab import files
files.upload()

# install 
!pip install wordcloud
!pip install tweet-preprocessor

# required libraries
import re
import pandas as pd
import numpy as np
import seaborn as sns
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS 
from textblob import TextBlob 
from nltk.tokenize import word_tokenize
from nltk.stem.porter import *
from textblob import TextBlob
import preprocessor as p

df=pd.read_csv('tweets_keywords1.csv')
df.head()

"""### for word clouds"""

# generate word cloud function
def generate_wordcloud(lists):
  comment_words = '' 
  stopwords = set(STOPWORDS) 
    
  # iterate through the words 
  for val in lists: 
        
      # typecaste each val to string 
      val = str(val) 
    
      # split the value 
      tokens = val.split() 

      comment_words += " ".join(tokens)+" "
    
  wordcloud = WordCloud(width = 800, height = 800, 
                  background_color ='white', 
                  stopwords = stopwords, 
                  min_font_size = 10).generate(comment_words) 
    
  # plot the WordCloud image                        
  plt.figure(figsize = (8, 8), facecolor = None) 
  plt.imshow(wordcloud) 
  plt.axis("off") 
  plt.tight_layout(pad = 0) 
    
  plt.show()

def extract(pat,lists):

  if(pat=='caps'):
    caps=[]
    for i in lists:
      sent=str(i)
      k=(re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', sent,re.MULTILINE))
      for j in k:
        caps.append(j)
    return caps

  elif pat=='excla':
    excla=[]
    for i in lists:
      i=re.findall(r'\w+!',i)
      for j in i:
        excla.append(j)
    return excla
  
  elif pat=='ques':
    ques=[]
    for i in lists:
      i=re.findall(r'\w+?',i)
      for j in i:
        ques.append(j)
    return ques

caps=extract("caps",df['text_with_converted_emoji'])
exc=extract("excla",df['text_with_converted_emoji'])
ques=extract("ques",df['text_with_converted_emoji'])

print("capital words")
generate_wordcloud(caps)
print("exclamation")
generate_wordcloud(exc)
print("question")
generate_wordcloud(ques)

"""## preprocess"""

def clean_tweets(lists):
  
  #convert to lower text
  for i in range(len(lists)):
    lists[i]=lists[i].lower()

  #remove numbers 
  
    lists[i] = re.sub(r'\d+','',lists[i])

  #remove punctuations
  
    lists[i]=re.sub(r"[\"#$%&()*+,-./:;<=>?@[\]^_`{|}~]", "",lists[i], flags=re.I)
    #lists[i] = re.sub(r'[\']'," ",lists[i],flags=re.I) #change it's to it s but isn't to isn  t 

  #remove multiple spaces
    lists[i]=re.sub(r"\s+"," ", lists[i], flags = re.I)

  #remove URL
    
    lists[i]=p.clean(lists[i])
  
  #using text blob remove the spelling mistakes
    lists[i]=TextBlob(lists[i]).correct()
    
    print(i)

df['clean']=df['text_with_converted_emoji']
clean_tweets(df['clean'])

# 6:30- 6:46 : 476
# 15 mins - 476 tweets

df['sentiment_polarity']=df['clean']
df['sentiment_subjectivity']=df['clean']

for i in range(len(df['clean'])):
  df['sentiment_polarity'][i]=TextBlob(df['sentiment_polarity'][i]).sentiment.polarity
  df['sentiment_subjectivity'][i]=TextBLob(df['sentiment_subjectivity'][i]).sentiment.sentiment_subjectivity

stemmer = PorterStemmer()
for i in range(len(df['clean'])):
  for j in range(len(df['clean'][i])):
    df['clean'][i]=' '.join(stemmer.stem(j))

"""# Trying to improve the speed of downloading etc"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords0.csv')

df1=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords1.csv')

df2=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords2.csv')

df2.shape

df3=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords3.csv')

df3.shape

df4=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords4.csv')

df4.shape

df5=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords5.csv')

df5.shape

df6=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords6.csv')

df6.shape

df7=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords7.csv')

df7.shape

df8=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords8.csv')

df8.shape

dfn=pd.concat([df,df1,df2,df3,df4,df5,df6,df7,df8])

dfn.size

dfn.to_csv('0to8merge.csv')

df_3=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_3.csv')

df_3.shape

df_4=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_4.csv')

df_4.shape

df_5=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_5.csv')

df_5.shape

df_6=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_6.csv')

df_6.shape

df_7=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_7.csv')

df_8=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_8.csv')

"""##...."""



dfcn=pd.concat([df_3,df_4,df_5,df_6,df_7,df_8])

df_new=pd.concat([dfcn,dfn])

dfcn.shape

dfn.shape

df_new.to_csv('merged_tweets.csv')

df_new.shape

df_new.size #55 MB

us0=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords0.csv')

us1=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords1.csv')

us2=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords2.csv')

us3=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords3.csv')

us4=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords4.csv')

us5=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords5.csv')

us6=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords6.csv')

us7=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords7.csv')

us8=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords8.csv')

us_3=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_3.csv')

us_4=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_4.csv')

us_5=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_5.csv')

us_6=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_6.csv')

us_7=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_7.csv')

us_8=pd.read_csv('/content/gdrive/My Drive/GSTN/user_mentions_keywords_8.csv')

usn=pd.concat([us0,us1,us2,us3,us4,us5,us6,us7,us8,us_3,us_4,us_5,us_6,us_7,us_8])

usn.to_csv('user_mentions_merge.csv')

usn.shape

usn.size



"""# Preprocessing"""

from google.colab import drive
drive.mount('/content/gdrive')

# install 
!pip install wordcloud
!pip install tweet-preprocessor

# required libraries
import re
import pandas as pd
import numpy as np
import seaborn as sns
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS 
from textblob import TextBlob 
from nltk.tokenize import word_tokenize
from nltk.stem.porter import *
from textblob import TextBlob
import preprocessor as p

df=pd.read_csv('/content/gdrive/My Drive/GSTN/merged_tweets.csv')

df.head()

a="yghjkl ghjkl ghjkl hbbhhhbhb https://t.co/m4IOLblwB2"
p.clean(a)

df.columns

df=df.drop(columns=['Unnamed: 0','Unnamed: 0.1'])

df.head()

"""### for word clouds"""

# generate word cloud function
def generate_wordcloud(lists):
  comment_words = '' 
  stopwords = set(STOPWORDS) 
    
  # iterate through the words 
  for val in lists: 
        
      # typecaste each val to string 
      val = str(val) 
    
      # split the value 
      tokens = val.split() 

      comment_words += " ".join(tokens)+" "
    
  wordcloud = WordCloud(width = 800, height = 800, 
                  background_color ='white', 
                  stopwords = stopwords, 
                  min_font_size = 10).generate(comment_words) 
    
  # plot the WordCloud image                        
  plt.figure(figsize = (8, 8), facecolor = None) 
  plt.imshow(wordcloud) 
  plt.axis("off") 
  plt.tight_layout(pad = 0) 
    
  plt.show()

def extract(pat,lists):

  if(pat=='caps'):
    caps=[]
    for i in lists:
      sent=str(i)
      k=(re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', sent,re.MULTILINE))
      for j in k:
        caps.append(j)
    return caps

  elif pat=='excla':
    excla=[]
    for i in lists:
      i=str(i)
      i=re.findall(r'\w+!',i)
      for j in i:
        excla.append(j)
    return excla
  
  elif pat=='ques':
    ques=[]
    for i in lists:
      i=str(i)
      i=re.findall(r'\w+?',i)
      for j in i:
        ques.append(j)
    return ques

caps=extract("caps",df['text_with_converted_emoji'])
exc=extract("excla",df['text_with_converted_emoji'])
ques=extract("ques",df['text_with_converted_emoji'])

print("capital words")
generate_wordcloud(caps)
print("exclamation")
generate_wordcloud(exc)
print("question")
generate_wordcloud(ques)

"""## cleaning"""

df.shape

df1=df.loc[0:400000]

df1.to_csv('merged_before1.csv')

df1.shape

df2=(df.loc[400001:800000]).reset_index()

df2.to_csv('merged_before2.csv')

df2.shape

df3=df.loc[800001:1283348]

df3.to_csv('merged_before_3.csv')

df3.shape

400001+400000+483347==1283348



"""# clean"""

def clean_tweets(lists):
  
  
  for i in range(len(lists)):
    text=lists[i]
    text=str(text)

  #remove URL
    text=p.clean(text)

  #convert to lower text
    text=text.lower()

  #remove numbers 
  
    text = re.sub(r'\d+','',text)

  #remove punctuations
  
    text=re.sub(r"[\"#$%&()*+,-./:;<=>?@[\]^_`{|}~]", "",text, flags=re.I)
    #lists[i] = re.sub(r'[\']'," ",lists[i],flags=re.I) #changing it's to it s but isn't to isn  t ,this can be dealt with by tokenizer..

  #remove multiple spaces
    text=re.sub(r"\s+"," ",text, flags = re.I)

  
  
  #using text blob remove the spelling mistakes
    #text=TextBlob(text).correct()

    lists[i]=type(lists[i])((text))
    print (i)

df2['clean']=df2['text_with_converted_emoji']
clean_tweets(df2['clean'])

df2.to_csv('preprocess_tweets2.csv')

"""#..."""

stemmer = PorterStemmer()
for i in range(len(df1['clean'])):
  for j in range(len(df1['clean'][i])):
    df1['clean'][i]=' '.join(stemmer.stem(str(j)))
  print(i)

from google.colab import drive
drive.mount('/content/drive')



len(set(stopwords.words('english')+['com']))

len(set(STOPWORDS))

from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())

print(X.shape)

df1['vector']=df['clean']

from nltk.tokenize import TweetTokenizer

df1.head()

from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(stop_words=set(STOPWORDS))

for i in range(len(df1['vector'])):
  df1['vector'][i]=vec.fit_transform([df1['vector']])

d



"""# merging the preprocessing files"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df_6=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_6.csv')

lists=[df0,df1,df2,df3,df4,df5,df6,df7,df8,df_3,df_4,df_5,df_6,df_7,df_8]

df_new=pd.concat(lists)

df_new.shape

df_new.to_csv('merged_preprocessed_tweets.csv')



"""# Emoji - text """

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df=pd.read_csv('/content/gdrive/My Drive/GSTN/merged_tweets.csv')

df.head().T

df['emoji_from_tweets'].count()

def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns

df=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords_8.csv')

missing_values_table(df)

# 2nd set of keywords has to be done again !! :(
# _4 also , _5 also,

df=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets0.csv')

from nltk.tokenize import TweetTokenizer

df['tokens']=df['clean']
for i in range(len(df['clean'])):
  df['tokens'][i]=TweetTokenizer().tokenize(df['clean'][i])

df['tokens'][2]

https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92

from gensim.models import Word2Vec

word_model = Word2Vec(df['tokens'], min_count=1,size= 50,workers=3, window =3, sg = 0)

word_model.most_similar('gst')[:5]

word_model

word_model.wv.similar_by_word('please',topn=20)

vocabulary = word_model.wv.vocab
print(len(vocabulary))

v1 = word_model.wv['fees']
v1
# vector representation of fees

https://www.ahmedbesbes.com/blog/sentiment-analysis-with-keras-and-word-2-vec

type(vocabulary)

a=vocabulary['fees']
print(a)

(word_model.wv.vectors).shape

matrix=(word_model.wv.vectors)

import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import sys
import logging
import time
import json
import os
import logging.config
from sklearn.cluster import KMeans
import numpy as np
ks = [2,4,7,10]


# track scores
sil_scores = []
inertias = []

# fit the models, save the scores from each run
for k in ks:
    logging.warning('fitting model for {} clusters'.format(k))
    model = KMeans(n_clusters=k,n_jobs=-1, random_state=123)
    model.fit(matrix)
    labels = model.labels_
    sil_scores.append(silhouette_score(matrix, labels))
    inertias.append(model.inertia_)

# plots
fig, ax = plt.subplots(2, 1, sharex=True)

plt.subplot(211)
plt.plot(ks, inertias, 'o--')
plt.ylabel('inertia')
plt.title('kmeans parameter search')

plt.subplot(212)

plt.plot(ks, sil_scores, 'o--')
plt.ylabel('silhouette score')
plt.xlabel('k');

"""for word2vec k means keywords 0
![image.png](data:image/png;base64,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)
"""

model = KMeans(n_clusters=4, max_iter=1000, random_state=True, init='k-means++').fit(matrix)

model.labels_

word_model.similar_by_vector(model.cluster_centers_[2], topn=10, restrict_vocab=None)

df['emoji_from_tweets'].count()

df.dropna(column='emoji_from_tweets')

df_new=df.copy()

df.columns

df_new.dropna(subset=['emoji_from_tweets'])[['text', 'text_with_converted_emoji']]
df_emoji=pd.DataFrame(df_new.query('emoji_from_tweets == emoji_from_tweets')[['text', 'text_with_converted_emoji']],columns=['text', 'text_with_converted_emoji'])

type(df_emoji)

df_emoji.head()

df_emoji['text'][25]

df_emoji['text_with_converted_emoji'][25]

""""@caabhishek2012 @theicai @PMOIndia @narendramodi @FinMinIndia @CaDurgeshkabra @scpatodia @neerajarora91 @nsitharamanoffc @nsitharaman @ianuragthakur @Anurag_Office @FinMinIndia  great u guys are making assessee aatmanirbhar ryt? Glad CA's r also citizen of india ,do something for CA's too.. to make them aatmanirbhar,ya fir bs GST aaye tb yaad aayenge ?Or for vivad se viswas me?😂😂""""



""""@caabhishek2012 @theicai @PMOIndia @narendramodi @FinMinIndia @CaDurgeshkabra @scpatodia @neerajarora91 @nsitharamanoffc @nsitharaman @ianuragthakur @Anurag_Office @FinMinIndia  great u guys are making assessee aatmanirbhar ryt? Glad CA's r also citizen of india ,do something for CA's too.. to make them aatmanirbhar,ya fir bs GST aaye tb yaad aayenge ?Or for vivad se viswas me?face_with_tears_of_joyface_with_tears_of_joy""""

from textblob import TextBlob

df['sentiment']=df['clean']
for i in range(len(df['clean'])):
  df['sentiment'][i]=TextBlob(str(df['sentiment'][i])).sentiment.polarity

# importing bokeh library for interactive dataviz
import bokeh.plotting as bp
from bokeh.models import HoverTool, BoxSelectTool
from bokeh.plotting import figure, show, output_notebook

# defining the chart
output_notebook()
plot_tfidf = bp.figure(plot_width=700, plot_height=600, title="A map of 10000 word vectors",
    tools="pan,wheel_zoom,box_zoom,reset,hover,previewsave",
    x_axis_type=None, y_axis_type=None, min_border=1)

# getting a list of word vectors. limit to 10000. each is of 200 dimensions
word_vectors = [word_model[w] for w in word_model.wv.vocab.keys()]

# dimensionality reduction. converting the vectors to 2d vectors
from sklearn.manifold import TSNE
tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
tsne_w2v = tsne_model.fit_transform(word_vectors)

# putting everything in a dataframe
tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])
tsne_df['words'] = word_model.wv.vocab.keys()

# plotting. the corresponding word appears when you hover on the data point.
plot_tfidf.scatter(x='x', y='y', source=tsne_df)
hover = plot_tfidf.select(dict(type=HoverTool))
hover.tooltips={"word": "@words"}
show(plot_tfidf)



"""# Tokenization, Vectorization"""

#libraries
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from textblob import TextBlob

from google.colab import drive
drive.mount('/content/gdrive')



df=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets3.csv')

#tokenizer function to be passed into the Tfidf vectorizer
def my_tokenizer(text):
    """
    Convert text to a list of tokens using NLTK's TweetTokenizer
    """
    tokenizer = TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=False)
    tokens = tokenizer.tokenize(text)
    return tokens

vectorizer = TfidfVectorizer(analyzer = 'word',tokenizer=my_tokenizer, stop_words=set(stopwords.words('english')))

#storing the values to tf-idf in the matrix
matrix=vectorizer.fit_transform(df['clean'].values.astype('U'))

# applying K Means 

# divide the data into 2 clusters (to be labelled as positive, negative - according to the mean value of the sentiment polarity)

model = KMeans(n_clusters=2, max_iter=1000, random_state=True, init='k-means++').fit(matrix)

y_pred = model.predict(matrix[0])
# plot the cluster assignments and cluster centers
X=matrix[0]
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap="plasma")
plt.scatter(model.cluster_centers_[:, 0],   
            model.cluster_centers_[:, 1],
            marker='^', 
            c=[0, 1], 
            s=100, 
            linewidth=2,
            cmap="plasma")
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

#sentiment polarity of the text (using textblob)
df['sentiment']=df['clean']
for i in range(len(df['clean'])):
  df['sentiment'][i]=TextBlob(str(df['sentiment'][i])).sentiment.polarity

# make a dataframe of the cluster , sentiment polarity, tweets text

lists=[]
for i in range(matrix.shape[0]):
  lists.append([(model.predict(matrix[i]))[0],df['sentiment'][i],df['text_with_converted_emoji'][i]])

df_new=pd.DataFrame(lists,columns=['cluster','sentiment','text'])

model.cluster_centers_

matrix.shape

type(matrix)

df_new.head()

# mean value >0 => positve cluster 
# mean value <0 => negative cluster
df_new.groupby(by='cluster').describe()

plt.figure(figsize=(7,7))

# See the range of sentiment value grouped in a cluster

ax = df_new.plot(kind="scatter",x="cluster",y="sentiment",figsize=(10,8))

df['sentiment_label']=df['sentiment']
for i in range(len(df['sentiment'])):
  if(df['sentiment'][i]>0.5):
    df['sentiment_label'][i]=1
  elif(df['sentiment'][i]<-0.5):
    df['sentiment_label'][i]=-1;
  else:
    df['sentiment_label'][i]=0

df.groupby('sentiment').describe()

df.shape

import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import sys
import logging
import time
import json
import os
import logging.config
from sklearn.cluster import KMeans
import numpy as np
ks = [2,4,10]


# track scores
sil_scores = []
inertias = []

# fit the models, save the scores from each run
for k in ks:
    logging.warning('fitting model for {} clusters'.format(k))
    model = KMeans(n_clusters=k,n_jobs=-1, random_state=123)
    model.fit(matrix)
    labels = model.labels_
    sil_scores.append(silhouette_score(matrix, labels))
    inertias.append(model.inertia_)

# plots
fig, ax = plt.subplots(2, 1, sharex=True)

plt.subplot(211)
plt.plot(ks, inertias, 'o--')
plt.ylabel('inertia')
plt.title('kmeans parameter search')

plt.subplot(212)

plt.plot(ks, sil_scores, 'o--')
plt.ylabel('silhouette score')
plt.xlabel('k');



"""#...."""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df0=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets0.csv')

df1=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets1.csv')

df2_0=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords2_0.csv')

df2_1=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords2_1.csv')

df2_2=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords2_2.csv')

df2_3=pd.read_csv('/content/gdrive/My Drive/GSTN/tweets_keywords2_3.csv')

df3=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets3.csv')

df4=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets4.csv')

df5=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets5.csv')

df6=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets6.csv')

df7=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets7.csv')

df8=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets8.csv')

df_3=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_3.csv')

df_4=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_4.csv')

df_5=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_5.csv')

df_6=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_6.csv')

df_7=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_7.csv')

df_8=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets_8.csv')

lists=[df0,df1,df2_0,df2_1,df2_2,df2_3,df3,df4,df5,df6,df7,df8,df_3,df_4,df_5,df_6,df_7,df_8]

df=pd.concat(lists)

df.shape

df.text.count()

i=df2_0.shape[0]+df2_1.shape[0]+df2_2.shape[0]+df2_3.shape[0]

print(i)

df_old=pd.read_csv('/content/gdrive/My Drive/GSTN/preprocess_tweets2.csv')

df_old.text.count()

df_old.topic.unique()

df2_0.topic.unique()

df2_1.topic.unique()

df_new=pd.concat([df,df_old])

df_new.to_csv('new_merged_preprocessed.csv')

len(df_new.tweet_id.unique())

df_new.count()

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import TweetTokenizer
#tokenizer to remove unwanted elements from out data like symbols and numbers
token =TweetTokenizer()
cv = CountVectorizer(stop_words='english',tokenizer = token.tokenize)
text_counts= cv.fit_transform(df0['text_with_converted_emoji'])
# this is BOW model

text_counts

import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
stops =  set(stopwords.words('english')+['com'])
co = CountVectorizer(stop_words=stops)
for x in df_old['text_with_converted_emoji']:
  counts = co.fit_transform(str(x).split())
pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)

!pip install emoji
import emoji
 
#extract emojis from the given text s into a
def extract_emojis(s):
  a=''.join(c for c in s if c in emoji.UNICODE_EMOJI)
  if(a==''):
    return None
  else:
    return a

!pip install emot
import re
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
 
# Function for converting emojis into word
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
    return text

s="hi Iam Hima 😂 😞"
print(extract_emojis(s))

print(convert_emojis(s))

#function to remove the emoji from text
def remove_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, " ")
    return text

remove_emojis(s)

convert_emojis(extract_emojis(s))

"""## Change the structure of the dataset !"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df=pd.read_csv('/content/gdrive/My Drive/GSTN/merged_tweets.csv')

df.columns

df=df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])

!pip install emoji
import emoji
 
#extract emojis from the given text s into a
def extract_emojis(s):
  a=''.join(c for c in s if c in emoji.UNICODE_EMOJI)
  if(a==''):
    return None
  else:
    return a

!pip install emot
import re
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
 
# Function for converting emojis into word
def convert_emojis(text):
    if(text==None):
      return None
    else:
      for emot in UNICODE_EMO:
          text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
      return text

#remove emoji from a text
def remove_emojis(text):
  if(text==''):
    return none
  else:
    for emot in UNICODE_EMO:
        text = text.replace(emot, " ")
    return text

df['only_text']=df['text']
df['emoji_converted']=df['emoji_from_tweets']

df['text'].shape

df.head()

for i in range(df['text'].shape[0]):
  df['only_text'][i]=remove_emojis(str(df['text'][i]))
  df['emoji_converted']=convert_emojis(str(df['emoji_from_tweets'][i]))
  print(i)

df.head()

